{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihoods we have used:\n",
    "1. regression: Gaussian\n",
    "2. count regression: \n",
    "    Poisson: lambda=e^f,  Poisson2: lambda=ln(1+e^f)\n",
    "3. binary classification: \n",
    "    Bernoulli: sigmoid,   Bernoulli2: Probit\n",
    "    \n",
    "For the sigmoid function, we use **expit** which is provided by *scipy.special*, because it is stable, fast and fairly accurate. Additionally, it is equivalent to sigmoid. For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv,norm,lstsq,cholesky\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import scipy.io as sio\n",
    "from sklearn import preprocessing\n",
    "import random,time,GPy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import loggamma,roots_hermitenorm,gamma,expit\n",
    "from scipy.special import ndtr as std_norm_cdf\n",
    "\n",
    "_sqrt_2pi = np.sqrt(2*np.pi)\n",
    "_lim_val = np.finfo(np.float64).max\n",
    "_lim_val_exp = np.log(_lim_val)\n",
    "\n",
    "def std_norm_pdf(x): # define a standard normal pdf(from GPy)\n",
    "    x = np.clip(x,-1e300,1e300)\n",
    "    return np.exp(-np.square(x)/2)/_sqrt_2pi\n",
    "\n",
    "def safe_exp(f):\n",
    "    clip_f = np.clip(f, -np.inf, _lim_val_exp)\n",
    "    return np.exp(clip_f)\n",
    "\n",
    "def safe_ln(x, minval=0.0000000001):\n",
    "    return np.log(x.clip(min=minval))\n",
    "\n",
    "def kernel(X1, X2=None, l=1.0, sigma_f=1.0,K='SE'):\n",
    "    '''\n",
    "    Isotropic squared exponential kernel. Computes \n",
    "    a covariance matrix from points in X1 and X2.    \n",
    "    Args:\n",
    "        X1: Array of m points (m x d).\n",
    "        X2: Array of n points (n x d).\n",
    "\n",
    "    Returns:\n",
    "        Covariance matrix (m x n).\n",
    "    '''\n",
    "    if K=='diag': return sigma_f**2*np.ones((X1.shape[0], 1))\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T);sqdist=np.absolute(sqdist)#.ravel()))\n",
    "    if K=='SE': return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist);\n",
    "    elif K=='Matern32': return sigma_f**2 * (1+3**0.5*np.sqrt(sqdist)/l) * np.exp(-3**0.5*np.sqrt(sqdist)/l);\n",
    "    elif K=='Matern52': return sigma_f**2 * (1+3**0.5*np.sqrt(sqdist)/l+5*sqdist/(3*l**2)) * np.exp(-5**0.5*np.sqrt(sqdist)/l);\n",
    "\n",
    "def MFE(true_labels, pred_labels):\n",
    "    '''\n",
    "    Calculating mean fraction error(MFE) for count regression, the math is used as follows:\n",
    "    MFE = mean( abs( (true_labels - pred_labels)./true_labels ) ) \n",
    "    Written by Kaikai\n",
    "    '''    \n",
    "    if true_labels.size != pred_labels.size:\n",
    "        print('The size of true_labels and pred_labels is supposed to be identical.')\n",
    "        return -1    \n",
    "    true_labels = true_labels.flatten().astype('double'); pred_labels = pred_labels.flatten().astype('double')\n",
    "    true_labels_temp = true_labels.copy()\n",
    "    if 0 in true_labels: # replace zero with a very small positive value in order to avoid dividing by zero\n",
    "        print('There are elements of zero value in true labels.')\n",
    "        true_labels_temp[true_labels==0] = 1\n",
    "    \n",
    "    MFE = np.mean( np.abs( (true_labels - pred_labels)/true_labels_temp ) )\n",
    "    return MFE\n",
    "\n",
    "def calc_vlb(m,V, a, lik='Gaussian'):\n",
    "    prior_u = a[0]; prior_f = a[1] # prior mean for inducing points    \n",
    "    A = a[2] # Knm*inv(Kmm)\n",
    "    Kmm = a[3]; Kmm_inv = a[4]; Kmn = a[5]; Knn_diag = a[6]; y = a[7] # the ground truth for training data\n",
    "    noise_var = a[8]\n",
    "    num_train = len(prior_f);num_inducing = len(prior_u)\n",
    "    m_q = prior_f + np.dot(A, (m-prior_u)) # Eq.(3a) in paper\n",
    "    v_q = ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:, None] # Eq.(3b) in paper\n",
    "#     v_q = np.absolute(v_q)\n",
    "    c1 = m - prior_u; c2 = np.dot(Kmm_inv, c1);#print(v_q[:20],min(v_q))\n",
    "    (Sign,LogDetKmm) = np.linalg.slogdet(Kmm); LogDetKmm = Sign*LogDetKmm\n",
    "    (SignV,LogDetV) = np.linalg.slogdet(V); LogDetV = SignV*LogDetV;#print(v_q[:50])     \n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));#vlb_lik = expit(y*f);print('yf',expit(y*f))\n",
    "        vlb_lik = np.sum( 1/_sqrt_2pi*np.dot( safe_ln(expit(y*f)) ,w) ) # sigmoid liklihood\n",
    "#         vlb_lik = np.sum( 1.0/np.sqrt(2*np.pi)*np.dot( np.log(std_norm_cdf(y*f)+1e-10) ,w) ) # Probit liklihood\n",
    "    elif lik=='Gaussian':\n",
    "        vlb_lik = -np.log(np.sqrt(2*np.pi*noise_var)) - np.sum((y-m_q)**2+v_q)/(2*noise_var)\n",
    "    elif lik=='Poisson':\n",
    "        vlb_lik = np.dot(y.T,m_q) - np.sum(loggamma(y+1)) - np.sum(np.exp(m_q+0.5*v_q))\n",
    "    elif lik=='Poisson2': # another link func: lambda=ln(1+e^f)\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));\n",
    "        term1 = -np.sum(loggamma(y+1)); term2 = -np.sum( 1/_sqrt_2pi*np.dot( np.log(1+safe_exp(f)),w) )\n",
    "        term3 = np.sum( 1/_sqrt_2pi*np.dot( y*safe_ln(np.log(1+safe_exp(f))),w) )\n",
    "        vlb_lik = term1 + term2 + term3\n",
    "    vlb_kl = 0.5*( LogDetKmm - LogDetV + np.dot(c1.T, c2) + np.trace(np.dot(Kmm_inv,V)) - len(prior_u) )\n",
    "    vlb = vlb_lik - vlb_kl\n",
    "    return vlb \n",
    "\n",
    "def get_init_hyperparameters(Z,Z_label,lik='Poisson',K='SE'):\n",
    "    # For all methods, initial variational parameters are found by running the Laplace approximation on the subset/active set.\n",
    "    dim_data = Z.shape[1]\n",
    "    if K=='SE': kern=GPy.kern.RBF(dim_data, variance=1.0, lengthscale=1.0);\n",
    "    elif K=='Matern32': kern=GPy.kern.Matern32(dim_data, variance=1.0, lengthscale=1.0);\n",
    "    elif K=='Matern52': kern=GPy.kern.Matern52(dim_data, variance=1.0, lengthscale=1.0);\n",
    "    \n",
    "    if lik=='Poisson': likelihood=GPy.likelihoods.Poisson();\n",
    "    elif lik=='Poisson2': likelihood=GPy.likelihoods.Poisson(GPy.likelihoods.link_functions.Log_ex_1());\n",
    "    elif lik=='Gaussian': likelihood=GPy.likelihoods.Gaussian();\n",
    "    elif lik=='Bernoulli': likelihood=GPy.likelihoods.Bernoulli();\n",
    "\n",
    "    laplace_inf = GPy.inference.latent_function_inference.Laplace()\n",
    "    model_lap = GPy.core.GP(X=Z, Y=Z_label, likelihood=likelihood, inference_method=laplace_inf, kernel=kern)\n",
    "    model_lap.optimize();\n",
    "    if K=='SE': length_scale=model_lap.rbf.lengthscale[0];signal_variance=model_lap.rbf.variance[0]\n",
    "    elif K=='Matern32': length_scale=model_lap.Mat32.lengthscale[0];signal_variance=model_lap.Mat32.variance[0]\n",
    "    elif K=='Matern52': length_scale=model_lap.Mat52.lengthscale[0];signal_variance=model_lap.Mat52.variance[0]\n",
    "    print(model_lap)\n",
    "    return model_lap,length_scale,signal_variance\n",
    "\n",
    "def Adam(theta, g_t, t, alpha=0.01, m_t=0, v_t=0, opt='minimize'):\n",
    "    beta_1 = 0.9; beta_2 = 0.999; epsilon = 1e-8     #initialize the values of the parameters    \n",
    "    m_t = beta_1*m_t + (1-beta_1)*g_t                #updates the moving averages of the gradient\n",
    "    v_t = beta_2*v_t + (1-beta_2)*(g_t*g_t)          #updates the moving averages of the squared gradient\n",
    "    m_cap = m_t/(1-(beta_1**t))                      #calculates the bias-corrected estimates\n",
    "    v_cap = v_t/(1-(beta_2**t))                      #calculates the bias-corrected estimates\n",
    "    if opt=='maximize':\n",
    "        theta = theta + (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    else:\n",
    "        theta = theta - (alpha*m_cap)/(np.sqrt(v_cap)+epsilon)    #updates the parameters\n",
    "    return theta, m_t, v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GH_quad(mu_star,std_star): # gauss_hermite_quad to calculate numerical integration, w and z denote weights and sample points, respectively. \n",
    "    z,w = roots_hermitenorm(n=50, mu=False); z,w=z[:,None],w[:,None]\n",
    "    z = np.kron(std_star,z.T) + mu_star\n",
    "    return z,w\n",
    "\n",
    "def calc_err(X, xStar, yTrue, l, sigma2, m, V,Kmm,Kmm_inv,noise_var=0.1,lik='Poisson',K='SE'):\n",
    "    num_xStar = xStar.shape[0]; Kmn = kernel(X, xStar, l=l, sigma_f=np.sqrt(sigma2),K=K)\n",
    "    Knn_diag = kernel(xStar, l=l, sigma_f=np.sqrt(sigma2),K='diag')+noise_var*np.eye(num_xStar)#np.eye(num_xStar)*sigma2+noise_var*np.eye(num_xStar)\n",
    "    A = np.dot(Kmn.T, Kmm_inv);\n",
    "    mu_star = np.dot(A,m); v2_star = Knn_diag + np.dot(A, np.dot(V-Kmm,A.T))\n",
    "    std_star = np.sqrt(np.diag(v2_star))[:,None]\n",
    "    if lik=='Bernoulli':\n",
    "        p = calc_Bernoulli_pred(mu_star,std_star,yTrue); \n",
    "        nll = -np.sum(safe_ln(p))\n",
    "        err = np.sum( np.where(p<0.5,1,0) )/num_xStar # if p<0.5, the corresponding prediction is not right\n",
    "        return err,nll\n",
    "    elif lik=='Gaussian':\n",
    "        mu, std = calc_Gauss_pred(mu_star,std_star,noise_var)\n",
    "        err = np.sum( (yTrue-mu)**2 );\n",
    "#         nll=0.5*np.dot(np.dot((yTrue-mu).T,np.diag(1/(std**2).ravel())),yTrue-mu)+np.sum(np.log(std))+num_xStar/2*np.log(2*np.pi)\n",
    "        mse = 1/num_xStar*err;# print((yTrue-mu).shape);#print(std.shape);print(((yTrue-mu)/std).shape)        \n",
    "#         nll = np.sum(np.log(std))+num_xStar/2*np.log(2*np.pi)+1/2*np.sum(((yTrue-mu)/std)**2)\n",
    "        nll = 1/2*np.sum(((yTrue-mu)/std)**2)\n",
    "#         nll=0.5*np.dot(np.dot((yTrue-mu).T,np.diag(1/(std**2).ravel())),yTrue-mu)+0.5*np.log(np.diag((std**2).ravel())+num_xStar/2*np.log(2*np.pi)\n",
    "        return mse,nll\n",
    "    else: # Poisson, Poisson2\n",
    "        y_min = min(yTrue); y_max = max(yTrue); #print(y_min,y_max)\n",
    "        pmf = calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik);#print(pmf.shape);#print(pmf[:3,:]);#print((yTrue-y_min).astype(int).shape)\n",
    "        nll = -np.sum(safe_ln(pmf[np.arange(len(pmf)),(yTrue-y_min).astype(int).ravel()]));#print(pmf[np.arange(len(pmf)),(yTrue-y_min).astype(int).ravel()])\n",
    "        res = np.argmax(pmf, axis=1)+y_min; err = MFE(yTrue, res)\n",
    "        return err,nll\n",
    "\n",
    "def calc_Gauss_pred(mu_star,std_star,noise_var):\n",
    "    return mu_star, std_star+np.sqrt(noise_var)\n",
    "\n",
    "def calc_Poisson_pred(mu_star,std_star,num_xStar,y_min,y_max,lik='Poisson'):\n",
    "    f,w = GH_quad(mu_star,std_star)\n",
    "    y_range = np.arange(y_min,y_max+1)\n",
    "    lik_func = {\n",
    "        'Poisson': lambda f,y: 1/gamma(y+1)*safe_exp(-safe_exp(f)+f*y),\n",
    "        'Poisson2': lambda f,y: 1/gamma(y+1)*expit(-f)*( safe_ln(1.0+safe_exp(f))**y )\n",
    "    }[lik]\n",
    "    poisson_lik = np.zeros((num_xStar,len(y_range)))\n",
    "    for i,y in enumerate(y_range):\n",
    "        poisson_lik[:,i] = 1/_sqrt_2pi*np.dot(lik_func(f,y),w).ravel()\n",
    "    return poisson_lik\n",
    "\n",
    "def calc_Bernoulli_pred(mu_star,std_star,y):\n",
    "#     v = std_star**2; kappa_v = (1+np.pi*v/8.0)**(-1/2) # Probit liklihood\n",
    "#     p = sigmoid(kappa_v*mu_star) # p(y=1|x_*,m,V)\n",
    "    f,w = GH_quad(mu_star,std_star) # sigmoid liklihood\n",
    "    p = 1/np.sqrt(2*np.pi)*np.dot( expit(f*y), w ) # p(y=1|x_*,m,V)\n",
    "    return p\n",
    "\n",
    "def calc_m_q(m, A, prior_u, prior_f):\n",
    "    return prior_f + np.dot(A, (m-prior_u))\n",
    "\n",
    "def calc_v_q(V, A, Kmm, Knn_diag):\n",
    "    return ( Knn_diag.ravel() + np.diag(np.dot(A, np.dot(V-Kmm, A.T))) )[:,None] # Eq.(3b) in paper\n",
    "\n",
    "def calc_rho(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/_sqrt_2pi*np.dot(  y*expit(-y*f) , w ) # sigmoid liklihood\n",
    "#         return 1.0/_sqrt_2pi*np.dot( y*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w ) # Probit liklihood        \n",
    "    elif lik=='Gaussian':\n",
    "        return 1.0/noise_var*(y-m_q)\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q) + y\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        y = np.tile(y,[1,f.shape[1]]); term2 = expit(f); t0 = safe_ln(1+safe_exp(f))# avoid dividing by zeros\n",
    "        d1_log_lik = np.zeros(t0.shape)\n",
    "        d1_log_lik[t0!=0] = (y[t0!=0]/t0[t0!=0] - 1)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d1_log_lik, w )\n",
    "    \n",
    "def calc_lambda(m_q, v_q, y, lik='Poisson', noise_var=0.1):\n",
    "    if lik=='Bernoulli':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q))\n",
    "        return 1.0/np.sqrt(2*np.pi)*np.dot(  -expit(y*f)*expit(-y*f), w ) # sigmoid\n",
    "#         return 1.0/_sqrt_2pi*np.dot( -std_norm_pdf(f)**2 / (std_norm_cdf(y*f)**2+1e-10) - y*f*std_norm_pdf(f) / (std_norm_cdf(y*f)+1e-10), w )\n",
    "    elif lik=='Gaussian':\n",
    "        return -1.0/noise_var*np.ones((len(m_q),1))\n",
    "    elif lik=='Poisson':\n",
    "        return -np.exp(m_q + 0.5*v_q)\n",
    "    elif lik=='Poisson2':\n",
    "        f,w = GH_quad(m_q,np.sqrt(v_q));y = np.tile(y,[1,f.shape[1]]);\n",
    "        term2 = expit(f)*expit(f); t0 = safe_ln(1+safe_exp(f));d2_log_lik = np.zeros(t0.shape)\n",
    "        d2_log_lik[t0!=0] = ((y[t0!=0]/t0[t0!=0]-1)*safe_exp(-f[t0!=0])-y[t0!=0]/(t0[t0!=0])**2)*term2[t0!=0]\n",
    "        return 1.0/_sqrt_2pi*np.dot(  d2_log_lik, w )\n",
    "\n",
    "def dVLb_dm(m, rho, prior_u, Kmm_inv, A,batch_size=None,num_train=None):\n",
    "    if batch_size is None: coef=1\n",
    "    else: coef=num_train/batch_size\n",
    "    dm = np.dot(A.T,rho)*coef - np.dot(Kmm_inv, m-prior_u) # Eq.(11a) in paper\n",
    "    return dm\n",
    "\n",
    "def dVLb_dL(L, lam, Kmm_inv, A, noise_var=1e-8,batch_size=None,num_train=None):# optimizing the cholesky factor L guarantees the PSD of V automatically\n",
    "    if batch_size is None: coef=1\n",
    "    else: coef=num_train/batch_size\n",
    "    dL = np.dot( np.dot( np.dot(A.T, np.diag(lam.ravel())), A ), L )*coef + inv(L+np.sqrt(noise_var)*np.eye(len(L))).T - np.dot(Kmm_inv, L)\n",
    "    return dL\n",
    "\n",
    "def SDSVI(X,Y,Kmm,Kmm_inv,Kmn,Knn_diag,m,L,V,prior_u,prior_f,noise_var,var,num_inducing,num_iter,lr,m_t,v_t,lik,batch_size):\n",
    "    num_train=X.shape[0]; indx = random.sample(range(num_train),batch_size); #Xb = X[indx, :]; \n",
    "    Yb_label = Y[indx];prior_f=prior_f[indx]\n",
    "    rows=np.arange(num_inducing); cols=np.asarray(indx, dtype=np.intp)\n",
    "    Kmn_sto=Kmn[np.ix_(rows, cols)]; A = np.dot(Kmn_sto.T, Kmm_inv);\n",
    "    Knn_diag=Knn_diag[cols];\n",
    "    m_q = calc_m_q(m, A, prior_u, prior_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);\n",
    "    rho = calc_rho(m_q, v_q,Yb_label,lik, noise_var); lam = calc_lambda(m_q, v_q,Yb_label,lik, noise_var)\n",
    "    var = np.hstack([m.flatten(), L.flatten()])\n",
    "    dm = dVLb_dm(m, rho, prior_u, Kmm_inv, A,batch_size,num_train)\n",
    "    dL = dVLb_dL(L, lam, Kmm_inv, A, noise_var,batch_size,num_train)\n",
    "    gradients = np.hstack([dm.flatten(), dL.flatten()])\n",
    "    var, m_t, v_t = Adam(var,gradients,num_iter,lr,m_t,v_t,opt='maximize')\n",
    "    m = var[:num_inducing][:,None] # variantional mean\n",
    "    L = var[num_inducing:].reshape(num_inducing,num_inducing) # variational variance V=L*L.T\n",
    "    V = np.dot(L, L.T)\n",
    "    return m,L,V,m_t,v_t\n",
    "\n",
    "def MCSSVI(X,Y,Kmm,Kmm_inv,Kmn,Knn_diag,m,V,inv_V,prior_u,prior_f,noise_var,var,num_inducing,num_iter,lik,batch_size):\n",
    "    num_train=X.shape[0];\n",
    "    indx = random.sample(range(num_train),batch_size);#Xb = X[indx, :]; \n",
    "    Yb_label = Y[indx];prior_f=prior_f[indx]\n",
    "    rows=np.arange(num_inducing); cols=np.asarray(indx, dtype=np.intp)\n",
    "    Kmn_sto=Kmn[np.ix_(rows, cols)]; A = np.dot(Kmn_sto.T, Kmm_inv);\n",
    "    Knn_diag=Knn_diag[cols];\n",
    "    m_q = calc_m_q(m, A, prior_u, prior_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);\n",
    "    rho = calc_rho(m_q, v_q,Yb_label,lik, noise_var); lam = calc_lambda(m_q, v_q,Yb_label,lik, noise_var)\n",
    "    d = A.T; gamma = -lam; #print('old m:',m[:1]); print('old V:',V[:2,:2])\n",
    "    step_size = num_iter**(-1);#V_old=V\n",
    "    interm_V = Kmm_inv+ num_train/batch_size * np.dot( np.dot(A.T, np.diag(gamma.ravel())), A) #intermediate parameter\n",
    "    inv_V = (1-step_size)*inv_V + step_size*interm_V\n",
    "    V = inv( inv_V )\n",
    "    interm_m = num_train/batch_size * np.dot(d, rho + np.dot(A,m)*gamma)\n",
    "    m = (1-step_size)*m + step_size*np.dot(V, interm_m) # assume prior_u=0\n",
    "#     m = np.dot(V, num_train/batch_size * np.dot(d, rho + np.dot(A,m)*gamma) );\n",
    "    return m,V,inv_V\n",
    "\n",
    "def HMCSSVI(X,Y,Kmm,Kmm_inv,Kmn,Knn_diag,m,V,inv_V,prior_u,prior_f,noise_var,var,num_inducing,num_iter,lr,m_t,v_t,lik,batch_size):\n",
    "    num_train=X.shape[0];\n",
    "    indx = random.sample(range(num_train),batch_size);#Xb = X[indx, :]; \n",
    "    Yb_label = Y[indx];prior_f=prior_f[indx]\n",
    "    rows=np.arange(num_inducing); cols=np.asarray(indx, dtype=np.intp)\n",
    "    Kmn_sto=Kmn[np.ix_(rows, cols)]; A = np.dot(Kmn_sto.T, Kmm_inv); Knn_diag=Knn_diag[cols];\n",
    "    m_q = calc_m_q(m, A, prior_u, prior_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);\n",
    "    rho = calc_rho(m_q, v_q,Yb_label,lik, noise_var); lam = calc_lambda(m_q, v_q,Yb_label,lik, noise_var)\n",
    "    d = A.T; gamma = -lam; #print('old m:',m[:1]); print('old V:',V[:2,:2])\n",
    "    step_size = num_iter**(-1);#V_old=V\n",
    "    interm_V = Kmm_inv+ num_train/batch_size * np.dot( np.dot(A.T, np.diag(gamma.ravel())), A) #intermediate parameter\n",
    "    inv_V = (1-step_size)*inv_V + step_size*interm_V\n",
    "    V = inv( inv_V )\n",
    "#     V = inv( (1-step_size)*V + step_size*interm_V )\n",
    "    dm = dVLb_dm(m, rho, prior_u, Kmm_inv, A,batch_size,num_train)\n",
    "    m, m_t, v_t = Adam(m,dm,num_iter,lr,m_t, v_t,opt='maximize')\n",
    "    return m,V,inv_V,m_t,v_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data(count regression) count_dataset_ucsdpeds1l_N4000_D30,count_dataset_abalone_N4177_D9,count_dataset_flares_N1065_D24\n",
    "mat_contents = sio.loadmat('count_dataset_epid_N6238_D17.mat')#count_dataset_epid_N6238_D17,count_dataset_bikehour2011_N8734_D49\n",
    "x = mat_contents['x']; y = mat_contents['y'] #count_dataset_segment_N2310_D19;class_dataset_madelon_N2600_D500;class_dataset_usps35_N1540_D256\n",
    "x = x.astype('double')#class_dataset_musk_N6598_D166;class_dataset_yeast_N1484_D8,reg_dataset_mg_N1385_D6\n",
    "#reg_dataset_cpusmall_N8192_D12,reg_dataset_spacega_N3107_D6\n",
    "# shuffle the data and split data into training set and test set\n",
    "data = np.concatenate((x,y), axis=1)\n",
    "np.random.shuffle(data)\n",
    "num_data = data.shape[0]; dim_data = data.shape[1] - 1;\n",
    "num_train = int(0.8*np.ceil(num_data)); num_test = num_data - num_train\n",
    "x_train = data[:num_train, :-1]; y_train = data[:num_train, -1];\n",
    "x_test = data[num_train:, :-1]; y_test = data[num_train:, -1];\n",
    "y_train = y_train[:, None]; y_test = y_test[:, None]\n",
    "# data Standardization with zero mean and unit variance\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "X_train = scaler.transform(x_train); X_test = scaler.transform(x_test)\n",
    "np.random.seed(6);num_inducing=400\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(X,Y,ix,X_test=None,y_test=None,max_iter=100,lr=1*10**-5,FPb_cond=1*10**0,stop_cond=1*10**-5,VLB_opt='GD',lik='Poisson',K='SE'):\n",
    "    num_train = X.shape[0];num_inducing=len(ix);num_test=X_test.shape[0]; \n",
    "    prior_u = np.zeros((num_inducing, 1)); prior_f = np.zeros((num_train, 1))\n",
    "    Z = X[ix, :]; Z_label = Y[ix] # Z is the inducing set\n",
    "    model_lap,ls,sf2 = get_init_hyperparameters(Z,Z_label,lik=lik,K=K)#(Z_label+1)/2    \n",
    "    # variational parameters from initialization\n",
    "    f_mean, f_var = model_lap._raw_predict(X) ; #print(ls,sf2)\n",
    "    if lik=='Gaussian': noise_var=model_lap.Gaussian_noise.variance[0] \n",
    "    else: noise_var=1*10**-8\n",
    "    #  we use variational mean from Laplace appr\n",
    "    m = f_mean[ix]; V = np.diag(f_var[ix].ravel()); L = cholesky(V);inv_V=np.diag(1/f_var[ix].ravel())\n",
    "    Kmm = kernel(Z, Z, l = ls, sigma_f = np.sqrt(sf2),K=K) + noise_var*np.eye(len(Z))\n",
    "    Kmm_inv = inv(Kmm); Kmn = kernel(Z, X, l = ls, sigma_f = np.sqrt(sf2),K=K); \n",
    "    Knn_diag =kernel(X, l = ls, sigma_f = np.sqrt(sf2),K='diag'); #sf2*np.ones((num_train, 1))\n",
    "    A = np.dot(Kmn.T, Kmm_inv) # Knm*inv(Kmm)    \n",
    "    a = (prior_u,prior_f,A,Kmm,Kmm_inv,Kmn,Knn_diag,Y,noise_var)\n",
    "    num_iter = 0; VLB = []; VLB_time = []; err = []; nll=[]; FPb_converge = False# True\n",
    "    var = np.hstack([m.flatten(), L.flatten()])\n",
    "    start_time = time.time(); VLB.append(-calc_vlb(m,V, a,lik)[0][0]);\n",
    "    res = calc_err(Z,X_test,y_test,ls,sf2,m,V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik,K=K)\n",
    "    err.append(res[0]); nll.append(res[1]);\n",
    "    VLB_time.append(time.time()-start_time);print('Before iterations, VLB:{:.6f} err:{:.6f}, nll:{:.6f}'.format(VLB[-1],err[-1],nll[-1]) )\n",
    "    m_t=0; v_t=0;batch_size=500;np.random.seed(7);\n",
    "    while 1:\n",
    "        num_iter = num_iter +1;#break\n",
    "        if VLB_opt in ['GD','FPi','FPb','FPi-mean']:\n",
    "            m_q = calc_m_q(m, A, prior_u, prior_f); v_q = calc_v_q(V, A, Kmm, Knn_diag);#print('m_q:',m_q[:10])\n",
    "            # According to Table 1 in paper, expectations of the derivatives wrt N(f|m,v) for Possion likelihood\n",
    "            rho = calc_rho(m_q, v_q, Y,lik, noise_var); lam = calc_lambda(m_q, v_q,Y,lik, noise_var);\n",
    "            \n",
    "        if VLB_opt=='SDSVI':\n",
    "            m,L,V,m_t,v_t=SDSVI(X,Y,Kmm,Kmm_inv,Kmn,Knn_diag,m,L,V,prior_u,prior_f,noise_var,var,num_inducing,num_iter,lr,m_t,v_t,lik,batch_size)\n",
    "            \n",
    "        if VLB_opt=='MCSSVI':\n",
    "            m,V,inv_V=MCSSVI(X,Y,Kmm,Kmm_inv,Kmn,Knn_diag,m,V,inv_V,prior_u,prior_f,noise_var,var,num_inducing,num_iter,lik,batch_size)\n",
    "        if VLB_opt=='HMCSSVI':\n",
    "            m,V,inv_V,m_t,v_t=HMCSSVI(X,Y,Kmm,Kmm_inv,Kmn,Knn_diag,m,V,inv_V,prior_u,prior_f,noise_var,var,num_inducing,num_iter,lr,m_t,v_t,lik,batch_size)\n",
    "        \n",
    "        if VLB_opt=='GD':\n",
    "            dm = dVLb_dm(m, rho, prior_u, Kmm_inv, A); dL = dVLb_dL(L, lam, Kmm_inv, A, noise_var)\n",
    "            gradients = np.hstack([dm.flatten(), dL.flatten()])\n",
    "            var, m_t, v_t = Adam(var,gradients,num_iter,lr,m_t, v_t,opt='maximize')\n",
    "            m = var[:num_inducing][:,None] # variantional mean\n",
    "            L = var[num_inducing:].reshape(num_inducing,num_inducing) # variational variance V=L*L.T\n",
    "            V = np.dot(L, L.T);\n",
    "            \n",
    "        elif VLB_opt=='FPi':\n",
    "            dm = dVLb_dm(m, rho, prior_u, Kmm_inv, A)\n",
    "            m, m_t, v_t = Adam(m,dm,num_iter,lr,m_t, v_t,opt='maximize') # print('old V',V[:1,:1])\n",
    "            V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A ));\n",
    "        elif VLB_opt=='FPb':            \n",
    "            if FPb_converge:\n",
    "                dm = dVLb_dm(m, rho, prior_u, Kmm_inv, A); #print('old m:',m[:1]);\n",
    "                m, m_t, v_t = Adam(m,dm,num_iter,lr,m_t, v_t,opt='maximize')\n",
    "            else:\n",
    "                V = inv(Kmm_inv-np.dot( np.dot(A.T, np.diag(lam.ravel())), A))\n",
    "                \n",
    "        elif VLB_opt=='FPi-mean':\n",
    "            d = A.T; gamma = -lam; #print('old m:',m[:1]); print('old V:',V[:2,:2])\n",
    "            V = inv(Kmm_inv+np.dot( np.dot(A.T, np.diag(gamma.ravel())), A))\n",
    "            m = np.dot(V, np.dot(d, rho + np.dot(A,m)*gamma) );\n",
    "\n",
    "        VLB.append(-calc_vlb(m,V, a,lik)[0][0]); VLB_time.append(time.time()-start_time)\n",
    "        res = calc_err(Z,X_test,y_test,ls,sf2,m,V,Kmm,Kmm_inv,noise_var=noise_var,lik=lik,K=K)\n",
    "        err.append(res[0]); nll.append(res[1])\n",
    "        if num_iter>0:\n",
    "            delta_vlb = abs(VLB[-1]-VLB[-2])\n",
    "            if num_iter%1 == 0:\n",
    "                print('iter:{}, delta_VLB:{:.6f}, negVLB:{:.6f}, err:{:.6f}, nll:{:.6f}'.format(num_iter, VLB[-2]-VLB[-1], VLB[-1], err[-1],nll[-1]));\n",
    "            if VLB_opt=='FPb' and delta_vlb<=FPb_cond:\n",
    "                FPb_converge = not FPb_converge; #print('m or V converged')\n",
    "            if num_iter>5 and delta_vlb<=stop_cond:\n",
    "                if np.average(VLB[-2:])<np.average(VLB[-4:]):\n",
    "                    if num_iter>=max_iter: print('It has reached the maximum number of iterations.');break\n",
    "                    else:continue\n",
    "                else: print('After {} iterations it converged: delta_VLB:{:.6f}, negVLB:{:.6f}, err:{:.6f}'.format(num_iter,delta_vlb,VLB[-1],err[-1]) );break                \n",
    "            if num_iter>22 and np.average(VLB[-6:])>=np.average(VLB[-12:]): print('negVLB did not decrease any more.'); break;            \n",
    "            elif num_iter==max_iter:\n",
    "                print('It has reached the maximum number of iterations, i.e. {}, with delta_VLB:{:.6f}, negVLB:{:.6f} and err:{:.6f}'.format(max_iter,delta_vlb,VLB[-1],err[-1]));break            \n",
    "    return Z,Z_label, VLB, ls, sf2, m, V,noise_var, Kmm, Kmm_inv, A,VLB_time,err,nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "k='SE';lik='Poisson2';lr=5*10**-2#0.5*10**-2;HMCSSVI\n",
    "# FPimSD = Train(X_train,y_train,ix,X_test,y_test,max_iter=200,lr=lr,VLB_opt='SDSVI',lik=lik,K=k)#Gaussian,Bernoulli,Poisson2,SDSVI\n",
    "# Zs,Zs_label, VLBSD, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_timeSD,errSD,nllSD=FPimSD#Matern52,SE,Matern32\n",
    "# FPim1 = Train(X_train,y_train,ix,X_test,y_test,max_iter=200,lr=lr,VLB_opt='GD',lik=lik,K=k)#Gaussian,Bernoulli,Poisson2,SDSVI\n",
    "# Zs,Zs_label, VLB1, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time1,err1,nll1=FPim1#Matern52,SE,Matern32\n",
    "# FPim2 = Train(X_train,y_train,ix,X_test,y_test,max_iter=300,lr=lr,VLB_opt='FPb',lik=lik,K=k)#Gaussian,Bernoulli\n",
    "# Zs,Zs_label, VLB2, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time2,err2,nll2=FPim2#Matern52,SE\n",
    "# FPim3 = Train(X_train,y_train,ix,X_test,y_test,max_iter=200,lr=lr,VLB_opt='FPi',lik=lik,K=k)#Gaussian,Bernoulli\n",
    "# Zs,Zs_label, VLB3, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time3,err3,nll3=FPim3#Matern32,SE\n",
    "# FPH = Train(X_train,y_train,ix,X_test,y_test,max_iter=200,lr=lr,VLB_opt='HMCSSVI',lik=lik,K=k)#Gaussian,Bernoulli,Poisson2,SDSVI\n",
    "# Zs,Zs_label, VLBH, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_timeH,errH,nllH=FPH#Matern52,SE,Matern32\n",
    "FPim4 = Train(X_train,y_train,ix,X_test,y_test,max_iter=150,lr=lr,VLB_opt='FPi-mean',lik=lik,K=k)#Gaussian,Bernoulli\n",
    "Zs,Zs_label, VLB4, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_time4,err4,nll4=FPim4#Matern32,SE\n",
    "FPimSDm = Train(X_train,y_train,ix,X_test,y_test,max_iter=150,lr=lr,VLB_opt='MCSSVI',lik=lik,K=k)#Gaussian,Bernoulli,Poisson2,SDSVI\n",
    "Zs,Zs_label, VLBSDm, length_scale, sigma2, m, V,noise_var, Kmm, Kmm_inv,A,VLB_timeSDm,errSDm,nllSDm=FPimSDm#Matern52,SE,Matern32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"negVLB\");\n",
    "par.set_ylabel(\"err\")\n",
    "p0, = host.plot(VLB_time4, VLB4, 'bo-', lw=1, markersize=5, label=\"negVLB FPm\")\n",
    "p1, = host.plot(VLB_timeSDm, VLBSDm, 'ro-', lw=1, markersize=5, label=\"negVLB MC-SSVI\")\n",
    "p2, = par.plot(VLB_time4, err4, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"err FPm\")\n",
    "p3, = par.plot(VLB_timeSDm, errSDm, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"err MC-SSVI\")\n",
    "# p0, = host.plot(VLB_time1, VLB1, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "# p1, = host.plot(VLB_timeSD, VLBSD, 'ro-', lw=1, markersize=5, label=\"negVLB S-DSVI\")\n",
    "# p2, = par.plot(VLB_time1, err1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"err GD\")\n",
    "# p3, = par.plot(VLB_timeSD, errSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"err S-DSVI\")\n",
    "# p0, = host.plot(VLB_time3, VLB3, 'bo-', lw=1, markersize=5, label=\"negVLB FPi\")\n",
    "# p1, = host.plot(VLB_timeH, VLBH, 'ro-', lw=1, markersize=5, label=\"negVLB H-MC-SSVI\")\n",
    "# p2, = par.plot(VLB_time3, err3, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"err FPi\")\n",
    "# p3, = par.plot(VLB_timeH, errH, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"err H-MC-SSVI\")\n",
    "\n",
    "# p0, = host.plot(VLB_time1, VLB1, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "# p1, = host.plot(VLB_timeSD, VLBSD, 'ro-', lw=1, markersize=5, label=\"negVLB S-DSVI\")\n",
    "# p2, = par.plot(VLB_time1, nll1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"nll GD\")\n",
    "# p3, = par.plot(VLB_timeSD, nllSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll S-DSVI\")\n",
    "# p0, = host.plot(VLB_time3, VLB3, 'bo-', lw=1, markersize=5, label=\"negVLB FPi\")\n",
    "# p1, = host.plot(VLB_timeH, VLBH, 'ro-', lw=1, markersize=5, label=\"negVLB H-MC-SSVI\")\n",
    "# p2, = par.plot(VLB_time3, nll3, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"nll FPi\")\n",
    "# p3, = par.plot(VLB_timeH, nllH, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll H-MC-SSVI\")\n",
    "\n",
    "# p0, = par.plot(VLB_time1, nll1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"nll GD\")\n",
    "# p1, = par.plot(VLB_timeSD, nllSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll S-DSVI\")\n",
    "# p2, = par.plot(VLB_time1, err1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"err GD\")\n",
    "# p3, = par.plot(VLB_timeSD, errSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"err S-DSVI\")\n",
    "# p5, = par.plot(VLB_timeSD1, nllSD1, 'gs--', lw=1, markersize=5,fillstyle='none', label=\"nll SDSVI1\")\n",
    "\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p2.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "# leg.texts[4].set_color(p4.get_color());\n",
    "# leg.texts[5].set_color(p5.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"negVLB\");\n",
    "par.set_ylabel(\"nll\")\n",
    "p0, = host.plot(VLB_time4, VLB4, 'bo-', lw=1, markersize=5, label=\"negVLB FPm\")\n",
    "p1, = host.plot(VLB_timeSDm, VLBSDm, 'ro-', lw=1, markersize=5, label=\"negVLB MC-SSVI\")\n",
    "p2, = par.plot(VLB_time4, nll4, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"nll FPm\")\n",
    "p3, = par.plot(VLB_timeSDm, nllSDm, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll MC-SSVI\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p2.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "# host.set_ylabel(\"nll\");\n",
    "# p0, = host.plot(VLB_timeSD, nllSD, 'bo-', lw=1, markersize=5, label=\"S-DSVI\")\n",
    "# p1, = host.plot(VLB_timeH, nllH, 'go-', lw=1, markersize=5, label=\"H-MC-SSVI\")\n",
    "# p2, = host.plot(VLB_timeSDm, nllSDm, 'ro-', lw=1, markersize=5, label=\"MC-SSVI\")\n",
    "host.set_ylabel(\"err\")\n",
    "p0, = host.plot(VLB_timeSD, errSD, 'bo-', lw=1, markersize=5, label=\"S-DSVI\")\n",
    "p1, = host.plot(VLB_timeH, errH, 'go-', lw=1, markersize=5, label=\"H-MC-SSVI\")\n",
    "p2, = host.plot(VLB_timeSDm, errSDm, 'ro-', lw=1, markersize=5, label=\"MC-SSVI\")\n",
    "# p2, = par.plot(VLB_time1, nll1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"nll GD\")\n",
    "# p3, = par.plot(VLB_timeSD, nllSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll SD-SVI\")\n",
    "# p2, = par.plot(VLB_time1, err1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"err GD\")\n",
    "# p3, = par.plot(VLB_timeSD, errSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll SD-SVI\")\n",
    "# p5, = par.plot(VLB_timeSD1, nllSD1, 'gs--', lw=1, markersize=5,fillstyle='none', label=\"nll SDSVI1\")\n",
    "\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());#par.yaxis.get_label().set_color(p2.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "# leg.texts[3].set_color(p3.get_color())\n",
    "# leg.texts[4].set_color(p4.get_color());\n",
    "# leg.texts[5].set_color(p5.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"negVLB\");\n",
    "# par.set_ylabel(\"ERR\")\n",
    "par.set_ylabel(\"NLL\")\n",
    "\n",
    "p0, = host.plot(VLB_time1, VLB1, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "p1, = host.plot(VLB_timeSD, VLBSD, 'ro-', lw=1, markersize=5, label=\"negVLB SDSVI\")\n",
    "# p2, = host.plot(VLB_timeSD1, VLBSD1, 'go-', lw=1, markersize=5, label=\"negVLB SDSVI1\")\n",
    "p2, = par.plot(VLB_time1, nll1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"nll GD\")\n",
    "p3, = par.plot(VLB_timeSD, nllSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll SDSVI\")\n",
    "# p2, = par.plot(VLB_time1, err1, 'bs--', lw=1, markersize=5,fillstyle='none', label=\"nll GD\")\n",
    "# p3, = par.plot(VLB_timeSD, errSD, 'rs--', lw=1, markersize=5,fillstyle='none', label=\"nll SDSVI\")\n",
    "# p5, = par.plot(VLB_timeSD1, nllSD1, 'gs--', lw=1, markersize=5,fillstyle='none', label=\"nll SDSVI1\")\n",
    "\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p2.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "# leg.texts[4].set_color(p4.get_color());\n",
    "# leg.texts[5].set_color(p5.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "VLB1 = (VLB1-min(VLB1))/(max(VLB1)-min(VLB1)); nll1 = (nll1-min(nll1))/(max(nll1)-min(nll1))\n",
    "VLB2 = (VLB2-min(VLB2))/(max(VLB2)-min(VLB2)); nll2 = (nll2-min(nll2))/(max(nll2)-min(nll2))\n",
    "VLB3 = (VLB3-min(VLB3))/(max(VLB3)-min(VLB3)); nll3 = (nll3-min(nll3))/(max(nll3)-min(nll3))\n",
    "VLB4 = (VLB4-min(VLB4))/(max(VLB4)-min(VLB4)); nll4 = (nll4-min(nll4))/(max(nll4)-min(nll4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"NLL\")#negVLB\n",
    "p0, = host.plot(VLB_time1, err1, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"error GD\")\n",
    "p1, = host.plot(VLB_time2, err2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"error FPb\")\n",
    "p2, = host.plot(VLB_time3, err3, 'go--', lw=1, markersize=5, fillstyle='none', label=\"error FPi\")\n",
    "p3, = host.plot(VLB_time4, err4, 'co--', markersize=5, lw=1, fillstyle='none', label=\"error FPi-mean\")\n",
    "p4, = par.plot(VLB_time1, nll1, 'bs-', lw=1, markersize=5, label=\"nll GD\")\n",
    "p5, = par.plot(VLB_time2, nll2, 'rs-', lw=1, markersize=5, label=\"nll FPb\")\n",
    "p6, = par.plot(VLB_time3, nll3, 'gs-', lw=1, markersize=5, label=\"nll FPi\")\n",
    "p7, = par.plot(VLB_time4, nll4, 'cs-', lw=1, markersize=5, label=\"nll FPi-mean\")\n",
    "# p8, = par.plot(VLB_time1, VLB1, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "# p9, = par.plot(VLB_time2, VLB2, 'ro-', lw=1, markersize=5, label=\"negVLB FPb\")\n",
    "# p10, = par.plot(VLB_time3, VLB3, 'go-', lw=1, markersize=5, label=\"negVLB FPi\")\n",
    "# p11, = par.plot(VLB_time4, VLB4, 'c+-', lw=1, markersize=5, label=\"negVLB FPi-mean\")\n",
    "\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "leg.texts[4].set_color(p4.get_color());\n",
    "leg.texts[5].set_color(p5.get_color())\n",
    "leg.texts[6].set_color(p6.get_color());\n",
    "leg.texts[7].set_color(p7.get_color())\n",
    "# leg.texts[8].set_color(p8.get_color());\n",
    "# leg.texts[9].set_color(p9.get_color())\n",
    "# leg.texts[10].set_color(p10.get_color());\n",
    "# leg.texts[11].set_color(p11.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "# p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson GD error\")\n",
    "# p1, = host.plot(VLB_time2, err2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 GD error\")\n",
    "# p2, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"Poisson GD negVLB\")\n",
    "# p3, = par.plot(VLB_time2, VLB2, 'ro-', lw=1, markersize=5, label=\"Poisson2 GD negVLB\")\n",
    "# p0, = host.plot(VLB_time_FPb, err_FPb, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPb error\")\n",
    "# p1, = host.plot(VLB_time_FPb2, err_FPb2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPb error\")\n",
    "# p2, = par.plot(VLB_time_FPb, VLB_FPb, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "# p3, = par.plot(VLB_time_FPb2, VLB_FPb2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "p0, = host.plot(VLB_time_FPi, err_FPi, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"Poisson FPi error\")\n",
    "p1, = host.plot(VLB_time_FPi2, err_FPi2, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"Poisson2 FPi error\")\n",
    "p2, = par.plot(VLB_time_FPi, VLB_FPi, 'bo-', lw=1, markersize=5, label=\"Poisson FPb negVLB\")\n",
    "p3, = par.plot(VLB_time_FPi2, VLB_FPi2, 'ro-', lw=1, markersize=5, label=\"Poisson2 FPb negVLB\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "host.set_xlabel(\"Training time(log)\")\n",
    "host.set_ylabel(\"Error\")\n",
    "par.set_ylabel(\"negVLB\")\n",
    "p0, = host.plot(VLB_time, err, 'bo--', lw=1, markersize=5, fillstyle='none', label=\"error GD\")\n",
    "p1, = host.plot(VLB_time_FPi, err_FPi, 'ro--', lw=1, markersize=5, fillstyle='none', label=\"error FPi\")\n",
    "p2, = host.plot(VLB_time_FPb, err_FPb, 'go--', lw=1, markersize=5, fillstyle='none', label=\"error FPb\")\n",
    "p3, = host.plot(VLB_time_FPim, err_FPim, 'co--', markersize=5, lw=1, fillstyle='none', label=\"error FPi-mean\")\n",
    "p4, = par.plot(VLB_time, VLB, 'bo-', lw=1, markersize=5, label=\"negVLB GD\")\n",
    "p5, = par.plot(VLB_time_FPi, VLB_FPi, 'ro-', lw=1, markersize=5, label=\"negVLB FPi\")\n",
    "p6, = par.plot(VLB_time_FPb, VLB_FPb, 'go-', lw=1, markersize=5, label=\"negVLB FPb\")\n",
    "p7, = par.plot(VLB_time_FPim, VLB_FPim, 'c+-', lw=1, markersize=5, label=\"negVLB FPi-mean\")\n",
    "host.set_xscale('log',basex=10)\n",
    "leg = plt.legend()\n",
    "host.yaxis.get_label().set_color(p0.get_color());par.yaxis.get_label().set_color(p0.get_color())\n",
    "leg.texts[0].set_color(p0.get_color());\n",
    "leg.texts[1].set_color(p1.get_color())\n",
    "leg.texts[2].set_color(p2.get_color());\n",
    "leg.texts[3].set_color(p3.get_color())\n",
    "leg.texts[4].set_color(p4.get_color());\n",
    "leg.texts[5].set_color(p5.get_color())\n",
    "leg.texts[6].set_color(p6.get_color());\n",
    "leg.texts[7].set_color(p7.get_color())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make some 1D training data(regression)\n",
    "num_train = 2000;num_test=700                         # 500 training poitns\n",
    "X_train = np.linspace(0, 10, num_train)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F = np.sin(X_train)                   # True function (f = sin(x))\n",
    "y_train = F + 0.01*np.random.randn(num_train)[:,None]  # Observations\n",
    "X_test = np.linspace(0, 10, num_test)[:,None]       # Inputs evenly spaced between 0 and 10\n",
    "F_test = np.sin(X_test)                   # True function (f = sin(x))\n",
    "y_test = F_test + 0.01*np.random.randn(num_test)[:,None]  # Observations\n",
    "np.random.seed(4);num_inducing=200\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "\n",
    "# We will plot our data as well\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# Plot 0 class in blue\n",
    "plt.plot(X[np.where(y == -1),0],X[np.where(y == -1),1],'bo', mew=0.5, alpha=0.5)\n",
    "# Plot 1 class in red\n",
    "plt.plot(X[np.where(y == 1),0],X[np.where(y == 1),1],'ro', mew=0.5, alpha=0.5)\n",
    "\n",
    "# Annotate plot\n",
    "plt.xlabel(\"$x_1$\"), plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Banana Dataset (red=1, blue=0)\")\n",
    "plt.axis(\"square\"), plt.xlim((-3, 3)), plt.ylim((-3, 3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from file, make sure banana.csv is in the same directory as this notebook\n",
    "data = np.genfromtxt('banana.csv', delimiter=',')\n",
    "\n",
    "# Dimension of data\n",
    "D = data.shape[1]-1\n",
    "n = data.shape[0]\n",
    "# Seperate our data (input) from its corresponding label output\n",
    "# .. note we have to rescale from [-1,1] to [0,1] for a Bernoulli distribution\n",
    "X, y = data[:,:D], data[:,-1][:, None]\n",
    "num_train = 3000; num_test = n - num_train\n",
    "X_train, y_train = X[:num_train,:], y[:num_train]\n",
    "X_test, y_test = X[:num_test,:], y[:num_test]\n",
    "np.random.seed(3);num_inducing=100 # randomly select active set and then keep them fixed\n",
    "ix = random.sample(range(num_train), num_inducing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
